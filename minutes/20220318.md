## Minutes - 18 March 2022

### General discussion

Tom's presentation on Time GANNs: [slides](https://github.com/stefanoschmidt1995/GWAnomalyDetection/blob/main/minutes/material/TimeGAN_presentation.pdf)
\
Latent space is interesting and latent space is supervised: this are nice thing to ask to our model

Robin and Tom: hard to get something to work. all the models just predict the average

Robin: random output in FD time series
\
Sarah: what does an anomaly would look like to a network trained with normal data?
\
Robin: should predict things as usual...

Tom: the generator is informed by real data in the alternate loop

Robin: there is a static label for the network. How do we set this?
\
Tom: might be useful in data augmentation (in multi class data)
\
Sarah: static labels can be used to store the auxiliary channels

Stefano: How does the model perform with a gaussian model (i.e. no time correlation)?
\
Tom: it learns to replicate the same randomness you input. Concern: will an anomaly in such situation be clearly distinguished from random noise?
\
Robin: you can tune the input random distribution to achieve the right thing. This is based on prior knowledge

Robin: showed some plots of an anomaly score in the FD timeseries predicted by the LSTM. You don't see anomalies at the time location of glitches
\
Sarah: inject blip glitches and see at which SNR you start to see anomalies

Sarah: you should use synthetic data to train whatever network an not real data, as they include garbage you don't want to train the network with. This is also useful to set a controlled environment for testing the ML models
\
Robin: easier training
\
Stefano: maybe change the psd with time

Two closing remarks:
1. Time GANN looks good as it allows for clustering + foresting
2. FD may not be the only/the best feature to use for anomaly detection. Eventually we should scale up to a	large number of features. 
This may be expensive to train bu can become feasible with small networks around
